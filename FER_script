#!/usr/bin/python3

import mediapipe as mp
import time
import os

execfile('FER_model.py')

# Цикл с выбором вывода модели
while True:
    model_type = input('Enter output type (class or va): ')

    # Цикл с проверкой правильности ввода
    if model_type == 'class' or 'va':
        break
    else:
        print ('TypeModelError')
        continue

# Конфигурация вывода с вебкамеры в формат MJPG с разрешением 640x480
os.system("bash -c 'v4l2-ctl --set-fmt-video=width=640,height=480,pixelformat=1'")

# Создание объекта захвата кадров вебкамеры
capture = cv2.VideoCapture(0, cv2.CAP_V4L2)
# Конфигурация захвата кадров с вебкамеры
capture.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'))

# Создание детектора лица из фреймворка MediaPipe
faceDetector = mp.solutions.face_detection

# Путь до предобученных весов
weight_path = f'weights/model_{model_type}_weights.h5'
# Создание модели с предобученными весами
model = FER_model(path_model_weights=weight_path, type_model=model_type)

# Менеджер контекста для детектора лиц и его конфигурация
with faceDetector.FaceDetection(
        model_selection=0, min_detection_confidence=0.7) as face_detection:
    # Основной цикл
    while (capture.isOpened()):
        start = time.time() # Время старта цикла для вычисления обработки кадров в секунду
        ret, frame = capture.read() # Чтение кадра с вебкамеры
        # Поиск лица на кадре
        faces = face_detection.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        # Если лицо не обнаружено основной цикл начинается заново
        if not faces.detections:
            frame = cv2.putText(frame, 'Face don`t recognition', (5, 240),
                    cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 1)
            cv2.imshow('frame', frame)
            if cv2.waitKey(33) == ord('q'):
                break
            continue
        # Цикл обраотки кадраи предсказания эмоции
        for id, detection in enumerate(faces.detections):
            # Координаты и размеры бокса с лицом на кадре
            bbox = detection.location_data.relative_bounding_box
            mul_h = frame.shape[0]
            mul_w = frame.shape[1]
            x = int(abs(bbox.xmin) * mul_w)
            y = int(abs(bbox.ymin) * mul_h)
            w = int(abs(bbox.width) * mul_w)
            h = int(abs(bbox.height) * mul_h)
            # Обрезка лица из кадра
            frame_pred = frame[y:y+h, x:x+w, :]
            # Обозначение оптимального расстояния до вебкамеры (размера бокса с лицом)
            if h > 180 and h < 230:
                color_bbox = (0, 255, 0)
            else:
                color_bbox = (0, 0, 255)
            # Получение номера эмоции или координат и названия эмоции
            emotion_number, emotion_name = model.get_emotion(frame_pred)
            # Добавление прямоугольника вокруг лица на кадре
            frame = cv2.rectangle(frame, (x, y), (x+w, y+h), color_bbox, 2)
            # Добавление текста с предсказанной эмоцией моделью
            frame = cv2.putText(frame, f'{emotion_number}: {emotion_name}', (x, y-15),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        totalTime = time.time() - start
        # Вычисление количества обработанных кадров в секунду
        fps = 1 / totalTime 
        frame = cv2.putText(
            frame, f'FPS: {int(fps)}', (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

        cv2.imshow('frame', frame) # Вывод полученного предобработанного кадра

        # Выход из основного цикла при нажатии 'q'
        if cv2.waitKey(33) == ord('q'):
            break

capture.release() # Закрытие объекта захвата кадров с вебкамеры
cv2.destroyAllWindows() # Закрытие окна с выводом
